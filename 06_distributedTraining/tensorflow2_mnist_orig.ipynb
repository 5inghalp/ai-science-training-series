{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c8be3",
   "metadata": {},
   "source": [
    "## Paramemters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f883ee2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "(mnist_images, mnist_labels), (x_test, y_test) = \\\n",
    "    tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "print(mnist_images.shape, mnist_labels.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n",
    "             tf.cast(mnist_labels, tf.int64))\n",
    ")\n",
    "test_dset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(x_test[..., tf.newaxis] / 255.0, tf.float32),\n",
    "             tf.cast(y_test, tf.int64))\n",
    ")\n",
    "\n",
    "nsamples = len(list(dataset))\n",
    "ntests = len(list(test_dset))\n",
    "\n",
    "dataset = dataset.repeat().shuffle(10000).batch(batch_size)\n",
    "test_dset  = test_dset.repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "for x, y in dataset.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "    for i in range(8):    \n",
    "        plt.subplot(181+i)\n",
    "        plt.imshow(x[i].numpy())\n",
    "        plt.xticks([]);plt.yticks([])\n",
    "        plt.title(y[i].numpy(), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cb938",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "opt = tf.optimizers.Adam(lr)\n",
    "\n",
    "checkpoint_dir = './checkpoints/tf2_mnist'\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    probs = mnist_model(x)\n",
    "    pred = tf.math.argmax(probs, axis=1)\n",
    "    for i in range(8):    \n",
    "        plt.subplot(181+i)\n",
    "        plt.imshow(x[i].numpy())\n",
    "        plt.xticks([]);plt.yticks([])\n",
    "        plt.title(\"%s(%s)\"%(pred[i].numpy(),y[i].numpy()), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125361fb",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1455a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = mnist_model(images, training=True)\n",
    "        loss_value = loss(labels, probs)\n",
    "        pred = tf.math.argmax(probs, axis=1)\n",
    "        equality = tf.math.equal(pred, labels)\n",
    "        accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    return loss_value, accuracy\n",
    "\n",
    "@tf.function\n",
    "def validation_step(images, labels):\n",
    "    probs = mnist_model(images, training=False)\n",
    "    pred = tf.math.argmax(probs, axis=1)\n",
    "    equality = tf.math.equal(pred, labels)\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    loss_value = loss(labels, probs)\n",
    "    return loss_value, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = nsamples//batch_size\n",
    "ntest_step = ntests//batch_size\n",
    "\n",
    "print(\"Number of training step: %d\" %nstep)\n",
    "print(\"Number of validation step: %d\"%ntest_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "metrics={}\n",
    "metrics['train_acc'] = []\n",
    "metrics['valid_acc'] = []\n",
    "metrics['train_loss'] = []\n",
    "metrics['valid_loss'] = []\n",
    "metrics['time_per_epoch'] = []\n",
    "for ep in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    training_acc = 0.0\n",
    "    tt0 = time.time()\n",
    "    for batch, (images, labels) in enumerate(dataset.take(nstep)):\n",
    "        loss_value, acc = training_step(images, labels)\n",
    "        training_loss += loss_value/nstep\n",
    "        training_acc += acc/nstep\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            checkpoint.save(checkpoint_dir)\n",
    "            print('Epoch - %d, step #%06d/%06d\\tLoss: %.6f' % (ep, batch, nstep, loss_value))\n",
    "    # Testing                                                                                                                                    \n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    for batch, (images, labels) in enumerate(test_dset.take(ntest_step)):\n",
    "        loss_value, acc = validation_step(images, labels)\n",
    "        test_acc += acc/ntest_step\n",
    "        test_loss += loss_value/ntest_step\n",
    "    tt1 = time.time()\n",
    "    print('Epoch - %d, train Loss: %.6f, train Acc: %.6f, val loss: %.6f, val Acc: %.6f %.6f seconds' % (ep, training_loss,\\\n",
    " training_acc, test_loss, test_acc, tt1 - tt0))\n",
    "    metrics['train_acc'].append(training_acc)\n",
    "    metrics['train_loss'].append(training_loss)\n",
    "    metrics['valid_acc'].append(test_acc)\n",
    "    metrics['valid_loss'].append(test_loss)\n",
    "    metrics['time_per_epoch'].append(tt1 - tt0)\n",
    "checkpoint.save(checkpoint_dir)\n",
    "t1 = time.time()\n",
    "print(\"Total training time: %s seconds\" %(t1 - t0))\n",
    "np.savetxt(\"metrics.dat\", np.array([metrics['train_acc'], metrics['train_loss'], metrics['valid_acc'], metrics['valid_loss\\\n",
    "'], metrics['time_per_epoch']]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57481815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.plot(metrics['train_loss'], label='Training')\n",
    "plt.plot(metrics['valid_loss'], label=\"Validation\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "plt.plot(metrics['train_acc'], label='Training')\n",
    "plt.plot(metrics['valid_acc'], label=\"Validation\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69634a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "for x, y in dataset.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "    probs = mnist_model(x)\n",
    "    pred = tf.math.argmax(probs, axis=1)\n",
    "    for i in range(8):    \n",
    "        plt.subplot(181+i)\n",
    "        plt.imshow(x[i].numpy())\n",
    "        plt.xticks([]);plt.yticks([])\n",
    "        plt.title(\"%s(%s)\"%(pred[i].numpy(),y[i].numpy()), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2ae74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2021-11-30",
   "language": "python",
   "name": "conda-2021-11-30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
