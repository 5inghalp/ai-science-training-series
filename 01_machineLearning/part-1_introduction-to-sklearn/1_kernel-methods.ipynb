{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods and Hyperparameter Optimization\n",
    "Kernel methods are a type of machine learning that is particularly sensitive to choice of hyperparameters (variables that control how an algorithm learns).\n",
    "Here, we'll show you how to adjust them with sklearn's hyperparameter optimization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, train_test_split\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Methods\n",
    "The [\"kernel\" trick](https://en.wikipedia.org/wiki/Kernel_method) is make a supervised learning problem easier by creating new input features based on pairwise similarities between a new input point and each point from the training set. The kernel creates a new set of $N$ features where each feature is the similarity between a certain entry and each of $N$ points from the training set. The similiarity function used to compute the new features is called _the kernel_.\n",
    "\n",
    "We will first demonstrate how the kernel can drastically simplify the learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(-2, 2.1, 1)\n",
    "y = (-1 <= x) & (x <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "\n",
    "ax.scatter(x, [0]*5, c=y, s=100, ec='k')\n",
    "ax.set_ylim(ax.get_ylim())\n",
    "for i in [-1.5, 1.5]:\n",
    "    ax.plot([i]*2, ax.get_ylim(), 'k--')\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel('$x$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this problem is not linearly separable. You need 2 rules to separate the two classes.\n",
    "\n",
    "Now, we apply a kernel trick to create a new set of features. We use the [\"RBF Kernel\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html)\n",
    "to generate new features. The RBF kernel uses a Gaussian function to create new features\n",
    "\n",
    "$f_i = k(x, x_i) = \\exp(-\\gamma(x - x_i)^2))$\n",
    "\n",
    "Applying the kernel function to all of the data yields the so-called, \"kernel matrix\" where\n",
    "\n",
    "$K_{ij} = k(x_i, y_j)$\n",
    "\n",
    "We will use this term in the remainder of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = rbf_kernel(x[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "\n",
    "ax.scatter(x, f[:, 2], c=y, s=100, ec='k')\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.plot(ax.get_xlim(), [0.2]*2, 'k--')\n",
    "\n",
    "ax.set_ylabel('$f_2$')\n",
    "ax.set_xlabel('$x$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now my points are linear separable based on one of my new features, $f_2$. We've made the learning problem easier by transforming the data to a new space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory: Kernel Ridge Regression\n",
    "You can also use Kernel methods for regression. Kernel ridge regression (KRR) creates a model of the functional form:\n",
    "\n",
    "$f(x) = \\sum_i c_i K(x, x_i)$\n",
    "\n",
    "where the sum is over all points in the training set and $x_i$ is training point $i$.\n",
    "\n",
    "The coefficients of this equation, $c_i$, are learned by minimizing\n",
    "\n",
    "$\\sum_i (y_i - f(x_i))^2 + \\alpha \\left\\Vert{\\bf c}\\right\\Vert^2$\n",
    "\n",
    "where the squared error to the training data is $\\sum_i (y_i - f(x_i))^2$,\n",
    "the sum of squares of the weights is $\\left\\Vert{\\bf c}\\right\\Vert^2$ (i.e., $L_2$-norm), \n",
    "and $\\lambda$ controls the tradeoff between minimizing model error and\n",
    "model complexity (larger weights lead to a more complex model).\n",
    "\n",
    "This equation has a convenient [analytic solution](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "based on the fact it involves solving a system of linear equations.\n",
    "The costly step in that equation is a matrix inversion, which can be solved efficiently \n",
    "using [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) - a \n",
    "convenience due to symmetry properties of the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a KRR Model using `scikit-learn`\n",
    "We will use the [`KernelRidge`](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html) method built into sklearn to explain how to use KRR. \n",
    "\n",
    "We will choose the \"RBF\" kernel introduced above for our example as it is particularly good at fitting non-linear functions and, conveniently, also very sensitive to hyperparmaeter choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KernelRidge(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two parameters we are interested in fitting are:\n",
    "1. `gamma`: Controls the width of the kernel functions (see $\\gamma$ in the equation above)\n",
    "1. `alpha`:  As in our formula above, controls complexity\n",
    "\n",
    "Make some initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 8\n",
    "X = np.random.uniform(0, 10, (n_points, 1))  # 2D array with one column\n",
    "y = -0.6 * X * (X - 11) + np.random.normal(size=(n_points, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the importance of hyperparameters, let's show how the model predictions change as a function of the kernel width (see `\\gamma` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(6.5, 2.5), sharey=True)\n",
    "\n",
    "x_plot = np.linspace(0, 10, 128)[:, None]\n",
    "for ax, g in zip(axs, [0.1, 1, 10]):\n",
    "    # Adjust parameter, fit and run model\n",
    "    krr.set_params(gamma=g)\n",
    "    krr.fit(X, y)\n",
    "    y_plot = krr.predict(x_plot)\n",
    "    \n",
    "    ax.scatter(X, y, color='teal', alpha=0.5)\n",
    "    ax.plot(x_plot, y_plot, 'k--')\n",
    "\n",
    "    ax.set_title(f'$\\gamma = {g}$')\n",
    "    ax.set_xlabel('$x$')\n",
    "axs[0].set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how larger values of gamma give \"sharper\" models. We can see a different effect with changing the `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(6.5, 2.5), sharey=True)\n",
    "\n",
    "krr.set_params(gamma=0.1)\n",
    "for ax, a in zip(axs, [0.1, 1, 10]):\n",
    "    # Adjust parameter, fit and run model\n",
    "    krr.set_params(alpha=a)\n",
    "    krr.fit(X, y)\n",
    "    y_plot = krr.predict(x_plot)\n",
    "    \n",
    "    ax.scatter(X, y, color='teal', alpha=0.5)\n",
    "    ax.plot(x_plot, y_plot, 'k--')\n",
    "\n",
    "    ax.set_title(f'$\\\\alpha = {a}$')\n",
    "    ax.set_xlabel('$x$')\n",
    "axs[0].set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alpha` instead controls fitness with the training set. \n",
    "\n",
    "Our challenge with using KRR is to find the right mix of parameters for each problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Hyperparameters\n",
    "Scikit-learn provides a convenient tool, [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), for optimizing hyperparameters that we will use for this exercise.\n",
    "\n",
    "It works by wrapping your model inside of class that tests many variations of your model to using a series of parameters you define. \n",
    "\n",
    "You must provide `GridSearchCV` with:\n",
    "1. A basic model with all the settings you do not want to change\n",
    "1. A grid of parameters you with to try out (all will be evaluated)\n",
    "1. A scheme to define how to score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    estimator=krr, # Set the estimator to be KRR\n",
    "    param_grid={'alpha': np.logspace(-8, 0, 32)},  # The range\n",
    "    cv=ShuffleSplit(test_size=0.25, random_state=1),  # Use 10, 75/25% splits\n",
    "    scoring='neg_mean_absolute_error',  # Score based on MAE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works just like an other machine learning model in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except that one of its outputs contains the scores over the many models we tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "\n",
    "ax.semilogx(np.array(gs.cv_results_['param_alpha']), -1*gs.cv_results_['mean_test_score'])\n",
    "\n",
    "ax.set_xlabel('$\\\\alpha$')\n",
    "ax.set_ylabel('MAE')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the best parameter is about $10^-3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the error can change by about a factor of 8 when we adjust $\\alpha$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn** Fit the model changing alpha using the ranges we used in the last example but with only 16 steps and using a logarithmic space between $10^{-4}$ and $10^1$ in 16 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double click for answer\n",
    "\n",
    "<div style='display: None'>\n",
    "gs = GridSearchCV(\n",
    "    estimator=krr, # Set the estimator to be KRR\n",
    "    param_grid={\n",
    "        'alpha': np.logspace(-8, 0, 16),\n",
    "        'gamma': np.logspace(-4, 1, 16),\n",
    "    },  # The range\n",
    "    cv=ShuffleSplit(test_size=0.25, random_state=1),  # Use 10, 75/25% splits\n",
    "    scoring='neg_mean_absolute_error',  # Score based on MAE\n",
    ")\n",
    "gs.fit(X, y)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the GridSearch model fit, you can use it like any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.5), sharey=True)\n",
    "# Run model with `.predict`\n",
    "y_plot = gs.predict(x_plot)\n",
    "\n",
    "ax.scatter(X, y, color='teal', alpha=0.5)\n",
    "ax.plot(x_plot, y_plot, 'k--')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the data really well and you now know about:\n",
    "1. The basics of Kernel methods\n",
    "1. How to fit hyperparameters in sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
