{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specific-preservation",
   "metadata": {},
   "source": [
    "# Machine Learning with Molecular Fingerprints\n",
    "The first type of representation we will demonstrate are \"fingerprints\" which describe a molecule with a list of binary features that, roughly, indicate whether a molecule has a certain substructure. \n",
    "\n",
    "Along the way, we are going to show you how to write your own \"Transformer\" classes that integrate computing features in your machine learning models and illustrate the importance of feature selection.\n",
    "\n",
    "**ALCF NOTE**: Make sure to select the \"rapids-21.10\" Jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc34f27-be09-48b7-b16e-8c31ae738f31",
   "metadata": {},
   "source": [
    "Configuration to work better on Jupyter at ALCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8836a-89ee-4dee-8c49-b85f04711ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import parallel_backend\n",
    "parallel_backend('multiprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-wilson",
   "metadata": {},
   "source": [
    "## Load Sample Data\n",
    "We are going to use the QM9 subset from previous problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('datasets/qm9.json.gz', lines=True)#.sample(5000)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-raise",
   "metadata": {},
   "source": [
    "## Efficiently Computing Fingerprints\n",
    "We are going to make a utility class that simplifies computing fingerprints using scikit-learn's [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) class. Building a class with BaseEstimator allows it to work with some of the other functionality from scikit-learn, such as the Pipeline.\n",
    "\n",
    "*Aside*: Read [this](https://www.w3schools.com/python/python_classes.asp) if you are unfamiliar with Python classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-graph",
   "metadata": {},
   "source": [
    "Step 1: A simple function for computing fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_morgan_fingerprints(smiles: str, fingerprint_length: int, fingerprint_radius: int):\n",
    "    \"\"\"Get Morgan Fingerprint of a specific SMILES string.\n",
    "    Adapted from: <https://github.com/google-research/google-research/blob/\n",
    "    dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750>\n",
    "    Args:\n",
    "      graph (str): The molecule as a SMILES string\n",
    "      fingerprint_length (int): Bit-length of fingerprint\n",
    "      fingerprint_radius (int): Radius used to compute fingerprint\n",
    "    Returns:\n",
    "      np.array. shape = [hparams, fingerprint_length]. The Morgan fingerprint.\n",
    "    \"\"\"\n",
    "    # Parse the molecule\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Compute the fingerprint\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(\n",
    "        molecule, fingerprint_radius, fingerprint_length)\n",
    "    arr = np.zeros((1,), dtype=np.bool_)\n",
    "\n",
    "    # ConvertToNumpyArray takes ~ 0.19 ms, while\n",
    "    # np.asarray takes ~ 4.69 ms\n",
    "    DataStructs.ConvertToNumpyArray(fingerprint, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_morgan_fingerprints('C', 4, 4)  # As an example, compute methane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorganFingerprintTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Class that converts SMILES strings to fingerprint vectors\"\"\"\n",
    "    \n",
    "    def __init__(self, length: int = 256, radius: int = 4):\n",
    "        self.length = length\n",
    "        self.radius = radius\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Do need to do anything\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Compute the fingerprints\n",
    "        \n",
    "        Args:\n",
    "            X: List of SMILES strings\n",
    "        Returns:\n",
    "            Array of fingerprints\n",
    "        \"\"\"\n",
    "        \n",
    "        fing = [compute_morgan_fingerprints(m, self.length, self.radius) for m in X]\n",
    "        return np.vstack(fing)\n",
    "m = MorganFingerprintTransformer(4, 4)\n",
    "m.transform(['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-arctic",
   "metadata": {},
   "source": [
    "Ok, we are now ready to use Morgan fingerprints easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-retention",
   "metadata": {},
   "source": [
    "## KNN Regressor\n",
    "The k-Nearest-Neighbor (KNN) regressor is one of the conceptually simplest ML algorithms. You find the $k$ entries that are most similar to a record and predict by taking the average values of their classes. One additional thing to know about it is that scikit-learn's [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) lets you configure the similarity metric.\n",
    "\n",
    "There are actually a [multitude of ways](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric) to measure distance between two vectors. Some are better suited for continuous values and others are good for lists of True/False values, such as fingerprints. Of these, the QSAR community [typically uses](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-015-0069-3) the [Jaccard/Tanimoto Distance](https://en.wikipedia.org/wiki/Jaccard_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-patent",
   "metadata": {},
   "source": [
    "Before testing, let's make a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fa560-1ed7-411e-be81-aedf485e1b4e",
   "metadata": {},
   "source": [
    "And precompute the featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.length = 256\n",
    "test_fng = m.transform(test_data['smiles_0'])\n",
    "train_fng = m.transform(train_data['smiles_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-costume",
   "metadata": {},
   "source": [
    "Let's fit a model to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KNeighborsRegressor(n_neighbors=3, metric='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0508a-bc3c-4c72-8589-9e2ef7a446d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "krr.fit(train_fng, train_data['bandgap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d2add-d8f9-43d8-866a-2876e5b4a900",
   "metadata": {},
   "source": [
    "Notice how fast the model trains! It isn't learning any parameters, just storing the training data in a way that it's easy to look up neighbors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba422cd-0396-46cf-896d-c3a8c704b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = krr.predict(test_fng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e339742-87f1-4c92-bec8-3481da6e00a2",
   "metadata": {},
   "source": [
    "Running the model, on the other hand, takes longer. Looking up the nearest neighbors for each point is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f071d3a-d546-46d8-b2f7-28f43e331739",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "\n",
    "r2 = r2_score(y_pred, test_data['bandgap'])\n",
    "ax.text(0.02, 0.9, f'$R^2 = {r2:.2f}$', transform=ax.transAxes)\n",
    "ax.scatter(y_pred, test_data['bandgap'])\n",
    "\n",
    "ax.set_xlabel('$E_g$, ML')  \n",
    "ax.set_ylim(ax.get_ylim())\n",
    "ax.set_xlim(ax.get_ylim())\n",
    "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
    "    \n",
    "ax.set_ylabel('$E_g$, True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5dfc8-522c-4017-89aa-bf110ed4a66c",
   "metadata": {},
   "source": [
    "Overall, the model doesn't do too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-excitement",
   "metadata": {},
   "source": [
    "## Faster KNN with GPU: cuML\n",
    "The Rapids project has made the ability to run many data science techniques, including ML algorithms, on the GPU. No mean feat!\n",
    "\n",
    "*WARNING*: These cells require you to run a GPU-enabled system. If you don't have one, skip ahead to \"Pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc7503-f970-4352-8c89-057ca92851bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAPIDS cuML kNN model\n",
    "import cudf, cuml\n",
    "from cuml.neighbors import KNeighborsRegressor as cuKNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df261ff9-e9be-4bed-ac37-232c3df1f7b7",
   "metadata": {},
   "source": [
    "The Rapids version of the model behaves just like the sklearn version, though it has some different distance metrics available. `braycurtis` is the closest to the Jaccard distance we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575eddaf-f9fe-4726-a055-7fc587719907",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cuml.neighbors.VALID_METRICS['brute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95d5d0-2f97-4e05-b427-a47b87c77c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cuKNeighbors(n_neighbors=3, metric='braycurtis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(train_fng, train_data['bandgap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_hat = model.predict(test_fng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ce2b5-6fda-4c47-a385-c89a8513ba73",
   "metadata": {},
   "source": [
    "The first time you run the cell above it will be slower because of initialization overhead. If you run it again it will be _much_ faster than the CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264e601-a0ed-4a08-b9d4-09845b162261",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "\n",
    "r2 = r2_score(y_hat, test_data['bandgap'])\n",
    "ax.text(0.02, 0.9, f'$R^2 = {r2:.2f}$', transform=ax.transAxes)\n",
    "ax.scatter(y_hat, test_data['bandgap'])\n",
    "\n",
    "ax.set_xlabel('$E_g$, ML')  \n",
    "ax.set_ylim(ax.get_ylim())\n",
    "ax.set_xlim(ax.get_ylim())\n",
    "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
    "    \n",
    "ax.set_ylabel('$E_g$, True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34f751-8b65-432c-a988-6154c3ef923a",
   "metadata": {},
   "source": [
    "It does just as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-saint",
   "metadata": {},
   "source": [
    "## Making a Pipeline\n",
    "Always a good plan for doing experiments as it lets you configure things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('fingerprint', MorganFingerprintTransformer()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=2, metric='jaccard', n_jobs=-1))  # n_jobs = -1 lets the model run all available processors\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c0ac-6139-449c-9814-b11449d7d7c0",
   "metadata": {},
   "source": [
    "This pipeline will take in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-china",
   "metadata": {},
   "source": [
    "## Evaluate the Effect of Training Set Size\n",
    "See how the model performs as we increase data with using two different fingerprint lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common pattern of work in ML is to try a few variations of parameters\n",
    "#  and assess their performance. I prefer to use a pattern where I loop\n",
    "#  over all of those parameters and then store them as a Pandas DataFrame.\n",
    "#  You'll see how to do that in this cell\n",
    "results = []\n",
    "for l in [32, 64, 128]:\n",
    "    model.set_params(fingerprint__length=l)\n",
    "    for s in tqdm([3, 10, 30, 100, 300, 1000, 3000], desc=f'l={l}'):  # Loop over different training set sizes\n",
    "        for i in range(16):  # Repeat the experiment\n",
    "            subset = train_data.sample(s)  # Downselect to the desired size\n",
    "\n",
    "            # Train and test the model\n",
    "            model.fit(subset['smiles_0'], subset['bandgap'])\n",
    "            y_pred = model.predict(test_data['smiles_0'])\n",
    "\n",
    "            # Store the results\n",
    "            #  Use a list of dictionaries, which I can convert to \n",
    "            #  a DataFrame very easily\n",
    "            results.append({\n",
    "                'length': l,\n",
    "                'train_size': s,\n",
    "                'iteration': i,\n",
    "                'r2_score': r2_score(test_data['bandgap'], y_pred),\n",
    "                'mae': mean_absolute_error(test_data['bandgap'], y_pred)\n",
    "            })\n",
    "results = pd.DataFrame(results)  # Converts to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-military",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 2.5))\n",
    "\n",
    "# Group loops over subsets with the same value for\n",
    "#  a certain column. \"gid\" is that value, and group\n",
    "#  is a DataFrame with that population of data\n",
    "for gid, group in results.groupby('length'):\n",
    "    # First, group by the training set size and compute the average over averages\n",
    "    #  The data is already grouped by length, so here we just need to compute the \n",
    "    #  average for all groups with the same training set size\n",
    "    #\n",
    "    #  You can do that in one line with Pandas:\n",
    "    #  https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n",
    "    group_stats = group.groupby('train_size').mean()\n",
    "    \n",
    "    ax.loglog(group_stats.index, group_stats['mae'], '--o', label=f'l={gid}')\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Train Size')\n",
    "ax.set_ylabel('MAE')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-plasma",
   "metadata": {},
   "source": [
    "Note how the difference between the performance of models depending on the fingerprint length increases with training set size. \n",
    "They are all effectively equivalent at dataset sizes of 3 training sets, but are very different at 3000 entries. \n",
    "\n",
    "The benefit for the increased fingerprint length is that it can disambuigate better between molecules and this becomes important\n",
    "when you have a greater number of molecules to distinguish between. \n",
    "This notion of \"larger datasets can benefit from more complex models\" is one of the big lessons of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a91e33-39fb-4f29-9f0b-9675bdb9fd28",
   "metadata": {},
   "source": [
    "## Better Models using Feature Selection\n",
    "One underlying problem with simple distance metrics, like molecular fingerprints, is that they are not tailored to a specific problem.\n",
    "As an illustrative example, consider measuring distances between points on the Earth based on latitude, longitude and altitude. \n",
    "The altitude is a good distance metric for predicting air pressure but not the time of a sunrise. \n",
    "You could refine the features by hand given your knowledge of the problem (always a good option if you can do it!), \n",
    "but it is also posible and easy to get machine learning to prune features for you.\n",
    "\n",
    "The core goal of feature selection is find a subset of additional features that produces the best model on some validation scheme.\n",
    "Naively, one could test all possibilities. However, that is a problem in cases with large number of features such as our 128-dimensional distance metric which has $2^{128} \\approx 10^{38}$ possible subsets.\n",
    "Approximate methods are needed.\n",
    "\n",
    "As the \"final exam\" for this tutorial, we are going to have you make one:\n",
    "1. Use the [RecursiveFeatureElimination (RFE)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) method to select features using a [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to rank the importance of features. Have the RFE remove 25% of the features each step.\n",
    "2. Add it to a pipeline that starts with the Morgan Fingerprint Transformer that produces 512 features.\n",
    "3. Use a GridSearchCV optimizer to fit how many features to select with RFE (I suggest 4 choices between 64 and 512) along with the number of neighbors in KNN (3 choices) using [ShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html) with a test fraction of 0.1 and only 1 fold.\n",
    "4. Evaluate its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18b01b-1843-47ca-978e-3a361f3e179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eedd67-7488-43f4-aee6-5ef4f92dccff",
   "metadata": {},
   "source": [
    "Double click for an answer.\n",
    "\n",
    "<div style='display: none'>\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "model = Pipeline([\n",
    "    ('fingerprint', MorganFingerprintTransformer(length=512)),\n",
    "    ('rfe', RFE(RandomForestRegressor(n_jobs=-1), step=0.25, n_features_to_select=32)),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=2, metric='jaccard', n_jobs=-1))\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    model,\n",
    "    {'rfe__n_features_to_select': [64, 128, 256, 512], 'knn__n_neighbors': [1, 2, 4]},\n",
    "    cv=ShuffleSplit(1, test_size=0.1),\n",
    "    verbose=2,\n",
    ").fit(train_data['smiles_0'], train_data['bandgap'])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803c009-ce04-4e40-b55b-cde219d529bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.5))\n",
    "\n",
    "y_pred = gs.predict(test_data['smiles_0'])\n",
    "r2 = r2_score(y_pred, test_data['bandgap'])\n",
    "ax.text(0.02, 0.9, f'$R^2 = {r2:.2f}$', transform=ax.transAxes)\n",
    "ax.scatter(y_pred, test_data['bandgap'])\n",
    "\n",
    "ax.set_xlabel('$E_g$, ML')  \n",
    "ax.set_ylim(ax.get_ylim())\n",
    "ax.set_xlim(ax.get_ylim())\n",
    "ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
    "    \n",
    "ax.set_ylabel('$E_g$, True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db5287-bfcb-4767-9739-0e15503fa689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-21.10",
   "language": "python",
   "name": "rapids-21.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
