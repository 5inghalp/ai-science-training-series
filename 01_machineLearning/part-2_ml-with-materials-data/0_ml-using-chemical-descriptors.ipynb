{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptor-Based ML\n",
    "An example of approaches used to do machine learning with chemical descriptors. \n",
    "\n",
    "We are going to use [Mordred](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y#Sec1), a Python library that wraps RDKit and other codes so that they work better with the Python data and ML libraries.\n",
    "\n",
    "**ALCF NOTE**: Make sure to change the kernel to \"rapids-21.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Dataset\n",
    "We are going to use a copy of the [QM9 dataset](https://www.nr2_scorecom/articles/sdata201422) for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('datasets/qm9.json.gz', lines=True).sample(1024)  # Do not need the full set to make a point.\n",
    "print(f'Loaded {len(data)} molecules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse as RDKit molecule objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mol'] = data['smiles_0'].apply(Chem.MolFromSmiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Calculators with Modred\n",
    "The key unit for working with Mordred is the [``Calculator``](http://mordred-descriptor.github.io/documentation/master/mordred.html#mordred.Calculator) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = Calculator(descriptors, ignore_3D=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modred has a lot of descriptors, and we are going to use quite a few of them to demonstrate some points about machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First 10 out of {len(calc.descriptors)} descriptors')\n",
    "calc.descriptors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = calc.pandas(data['mol'], nproc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Computed {len(desc.columns)} descriptors for {len(desc)} molecules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The main issue before using all of these features for machine learning, especially on small datasets, is the need to perform feature selection.\n",
    "\n",
    "There are few issues to be concerned about. \n",
    "\n",
    "For one, some of the descriptors are the same for all molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variation = [c for c, s in desc.std().items() if s == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(zero_variation)} ({len(zero_variation) / len(desc.columns) *100:.1f}%) features are constant for the dataset. Examples: {zero_variation[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these features may be improtant for some datasets but not ours. For example, our dataset contains only C, H, N, O and F so the number of boron atoms is not needed.\n",
    "\n",
    "We can safely drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.drop(columns=zero_variation, inplace=True)\n",
    "print(f'New shape: {desc.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to convert all of the values to numbers. It just makes things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in desc.columns:\n",
    "    desc[c] = pd.to_numeric(desc[c], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look for features that have missing values. \n",
    "\n",
    "*Dev Note*: We use the [`isnull`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html) and [`any`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.any.html) functions from Pandas to check if any rows of each column have a null value. In many cases, you can write complex loops in Pandas as simple efficient statements. So, if you start writing a loop over a column maybe check the Pandas docs for an example before getting too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = desc.isnull().any()\n",
    "print(f'Found {missing_values.sum()} columns with missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = desc.loc[:, ~missing_values]  # Gets only columns that do not (~ means not) have missing values\n",
    "print(f'New shape: {desc.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testings Components a Model Pipeline\n",
    "The data require a few more transformations before we can use them in a machine learning model\n",
    "\n",
    "### Adjust Features to the Same Scale\n",
    "First, we need to scale the data. If we randomly select a few columns, you will see that they vary wildly in values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to scale it. We are going to use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to adjust so that the mean is zero and the standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "feats = scaler.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"scaler\" object holds on to the means and scales of each feature so that we can then apply them to new data. You'll see where this is useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_[:8], scaler.scale_[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress Features\n",
    "Many of the features in the dataset are strongly correlated with each other. For larger datasets, the subtle differences between different features could be helpful in building models. For smaller datasets, the strong correlations could lead to overfitting (e.g., some algorithms will learn preferrentially from the signals that are overrepresented in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr = np.corrcoef(feats.T)  # Feats.T is the transpose, which allows us to easily compute the correlation between descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many of the features are correlated strongly with each other.\n",
    "\n",
    "There are [numerous ways](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) to compress a set of features to a smaller set of the features that contain roughly the same information. We will use [Principle Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for its relative simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA creates a new set of features that are linear combinations of the descriptors. One of the best things about PCA is that it can also show how how much of the data is explained by the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as we increase the number of features, we start to gradually explain more of the data.\n",
    "\n",
    "Let's pick 16 features for now to illustrate the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca.n_components = 16\n",
    "pca_feats = pca.fit_transform(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all mutually orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr = np.corrcoef(pca_feats.T)\n",
    "print(f'The largest correlation between feature 0 and all others is {max(cross_corr[0, 1:]):.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features\n",
    "The last issue is that not all of the features are correlated well with the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_score = [np.corrcoef(data['bandgap'], f)[0][1] for f in pca_feats.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(len(feat_score)), feat_score)\n",
    "\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_ylabel(f'Correlation with Band Gap')\n",
    "ax.set_xlabel('Feature ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last portion means that we need to consider even more feature selection to select only those features which are relevant to the problem being modeled.\n",
    "\n",
    "For that, we are going to use [LASSO](https://scikit-learn.org/stable/modules/linear_model.html#lasso) - a linear regression model that automatically includes only the most important features. The penalty used to determine how many features are used, $\\alpha$, must be set by experimenting with different values. Scikit-Learn has the [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) class that automatically select the best $\\alpha$ for you using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV().fit(pca_feats, data['bandgap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(range(len(feat_score)), lasso.coef_)\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.plot(ax.get_xlim(), [0, 0], 'k--', lw=1.)\n",
    "\n",
    "ax.set_ylabel(f'Correlation with Band Gap')\n",
    "ax.set_xlabel('Feature ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your configuration for this notebook, you may notice some of the coefficients are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "Scikit-Learn includes a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) function to make it easier to train models that contain multiple steps. It is a fantastic tool to become familiar with because, as we will demonstrate, it makes experimenting with model settings easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Make a holdout-set. Not that I split both the data and the descriptors at the same time, which means the same rows (i.e., molecules) get partitioned from each array. `train_data[0]` is the same record as `train_desc[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_desc, test_desc = train_test_split(data, desc, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Make a hold-out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=16)),\n",
    "    ('lasso', LassoCV())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.copy()  # Avoids some pandas issues\n",
    "test_data['16-ncomp-model'] = model.fit(train_desc, train_data['bandgap']).predict(test_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's show how to change the PCA from 16 to only 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(pca__n_components=8)  # Notation is [step name]__[parameter name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['8-ncomp-model'] = model.fit(train_desc, train_data['bandgap']).predict(test_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(6.5, 3.))\n",
    "\n",
    "# Plot both models\n",
    "for ax, col in zip(axs, ['8-ncomp-model', '16-ncomp-model']):\n",
    "    ax.set_title(f'{col.split(\"-\")[0]}-Parameter Model')\n",
    "    r2 = r2_score(test_data[col], test_data['bandgap'])\n",
    "    ax.text(0.02, 0.9, f'$R^2 = {r2:.2f}$', transform=ax.transAxes)\n",
    "    ax.scatter(test_data[col], test_data['bandgap'])\n",
    "\n",
    "# Make it look nicer\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('$E_g$, ML')\n",
    "    \n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.set_xlim(ax.get_ylim())\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), 'k--')\n",
    "    \n",
    "axs[0].set_ylabel('$E_g$, True')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that you get a better model with adding additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-21.10",
   "language": "python",
   "name": "rapids-21.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
